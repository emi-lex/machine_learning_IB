{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danon6868/BI-ml-course/blob/main/lecture_8_neural_networks_part2/homework/FC_NN_practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pSGPQS8Btmc"
      },
      "source": [
        "Всем привет! Сегодня вы впервые попробуете написать свою собственную нейронную сеть и попробовать ее обучить. Мы будем работать с картинками, но пока что не совсем тем способом, которым лучше всего это делать, но должно получиться неплохо.\n",
        "\n",
        "Будем работать с [датасетом](https://github.com/rois-codh/kmnist) `Kuzushiji-MNIST` (`KMNIST`). Это рукописные буквы, изображения имеют размер (28, 28, 1) и разделены на 10 классов, по ссылке можно прочитать подробнее."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "75HVAP_RFU7r"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46iQ8ixtEruP"
      },
      "source": [
        "## Загрузка данных"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6h1jVreJlV-"
      },
      "source": [
        "Сейчас мы будем использовать встроенные данные, но в реальности приходится писать свой класс для датасета (Dataset), у которого реализовывать несколько обязательных методов (напр, `__getitem__`), но это обсудим уже потом."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "s9L9Z02o_1bK"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "from torchvision.datasets import KMNIST\n",
        "\n",
        "\n",
        "# Превращает картинки в тензоры\n",
        "transform = torchvision.transforms.Compose(\n",
        "    [torchvision.transforms.ToTensor()])\n",
        "\n",
        "# Загрузим данные (в переменных лежат объекты типа `Dataset`)\n",
        "# В аргумент `transform` мы передаем необходимые трансформации (ToTensor)\n",
        "trainset = KMNIST(root=\"./KMNIST\", train=True, download=True, transform=transform)\n",
        "testset = KMNIST(root=\"./KMNIST\", train=False, download=True, transform=transform)\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V83E2vDrO9CC"
      },
      "source": [
        "Определим даталоадеры, они нужны, чтобы реализовывать стохастический градиентный спуск (то есть мы не хотим считывать в оперативную память все картинки сразу, а делать это батчами)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "oqC8XO8pO8Px"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "# Можно оставить таким\n",
        "batch_size = 256\n",
        "\n",
        "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-Ntp5sLoPyGx"
      },
      "source": [
        "Подумайте, как может влиять на скорость обучения параметр `batch_size`, почему вы так считаете?\n",
        "\n",
        "**Ответ:** чем меньше батч, тем быстрее обучение на самом батче, потому что объектов меньше. Суммарно на всех батчах обучение тоже будет быстрее, потому что асимптотика обучения на одном батче явно больше, чем линейная. То есть пусть f(n) --  это асимптотика обучения, у нас k батчей, тогда $k*f(n/k) < k*f(n)/k = f(n)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2vmM4KaHvrs"
      },
      "source": [
        "Посмотрим на какую-нибудь картинку:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "N-b-kFCYAoOP",
        "outputId": "d077a35b-f874-40c2-e1da-1f461c11172f"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.imshow(trainset[0][0].view(28, 28).numpy(), cmap=\"gray\")\n",
        "plt.axis(\"off\")\n",
        "plt.title(f\"Class is {trainset[0][1]}\", fontsize=16);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_MSY231Hzz9"
      },
      "source": [
        "### Задание 1. Смотрим на картинки\n",
        "\n",
        "**2** балла\n",
        "\n",
        "Нарисуйте на одном графике изображения всех 10 классов:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0tNdHM5JS6l"
      },
      "source": [
        "⣿⣿⣿⣿⣿⣿⠿⢋⣥⣴⣶⣶⣶⣬⣙⠻⠟⣋⣭⣭⣭⣭⡙⠻⣿⣿⣿⣿⣿\n",
        "⣿⣿⣿⣿⡿⢋⣴⣿⣿⠿⢟⣛⣛⣛⠿⢷⡹⣿⣿⣿⣿⣿⣿⣆⠹⣿⣿⣿⣿\n",
        "⣿⣿⣿⡿⢁⣾⣿⣿⣴⣿⣿⣿⣿⠿⠿⠷⠥⠱⣶⣶⣶⣶⡶⠮⠤⣌⡙⢿⣿\n",
        "⣿⡿⢛⡁⣾⣿⣿⣿⡿⢟⡫⢕⣪⡭⠥⢭⣭⣉⡂⣉⡒⣤⡭⡉⠩⣥⣰⠂⠹\n",
        "⡟⢠⣿⣱⣿⣿⣿⣏⣛⢲⣾⣿⠃⠄⠐⠈⣿⣿⣿⣿⣿⣿⠄⠁⠃⢸⣿⣿⡧\n",
        "⢠⣿⣿⣿⣿⣿⣿⣿⣿⣇⣊⠙⠳⠤⠤⠾⣟⠛⠍⣹⣛⣛⣢⣀⣠⣛⡯⢉⣰\n",
        "⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣷⡶⠶⢒⣠⣼⣿⣿⣛⠻⠛⢛⣛⠉⣴⣿⣿\n",
        "⣿⣿⣿⣿⣿⣿⣿⡿⢛⡛⢿⣿⣿⣶⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣷⡈⢿⣿\n",
        "⣿⣿⣿⣿⣿⣿⣿⠸⣿⡻⢷⣍⣛⠻⠿⠿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠿⢇⡘⣿\n",
        "⣿⣿⣿⣿⣿⣿⣿⣷⣝⠻⠶⣬⣍⣛⣛⠓⠶⠶⠶⠤⠬⠭⠤⠶⠶⠞⠛⣡⣿\n",
        "⢿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣷⣶⣬⣭⣍⣙⣛⣛⣛⠛⠛⠛⠿⠿⠿⠛⣠⣿⣿\n",
        "⣦⣈⠉⢛⠻⠿⠿⢿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡿⠿⠛⣁⣴⣾⣿⣿⣿⣿\n",
        "⣿⣿⣿⣶⣮⣭⣁⣒⣒⣒⠂⠠⠬⠭⠭⠭⢀⣀⣠⣄⡘⠿⣿⣿⣿⣿⣿⣿⣿\n",
        "⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣦⡈⢿⣿⣿⣿⣿⣿\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "x3j6Bsu9AoT6"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 2160x216 with 10 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "fig, axes = plt.subplots(1, 10, figsize=(30, 3))\n",
        "\n",
        "for i in range(10):\n",
        "    # print(i)\n",
        "    for trainobj in trainset:\n",
        "        if trainobj[1] == i:\n",
        "            axes[i].imshow(trainobj[0].view(28, 28).numpy(), cmap=\"gray\")\n",
        "            axes[i].axis(\"off\")\n",
        "            axes[i].set_title(f\"Class is {i}\", fontsize=16)\n",
        "            break\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLCfnFW-JtGx"
      },
      "source": [
        "### Задание 2. Строим свой первый MLP\n",
        "\n",
        "**4** балла\n",
        "\n",
        "MLP (multilayer perceptron) или нейронная сеть из полносвязных (линейных) слоев, это мы уже знаем.\n",
        "\n",
        "Опишите структуру сети: 3 полносвязных слоя + функции активации на ваш выбор. **Подумайте** про активацию после последнего слоя!\n",
        "\n",
        "Сеть на выходе 1 слоя должна иметь 256 признаков, на выходе из 2 128 признаков, на выходе из последнего столько, сколько у вас классов.\n",
        " \n",
        "https://pytorch.org/docs/stable/nn.html?highlight=activation#non-linear-activations-weighted-sum-nonlinearity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "dhYBvQIXJdSz"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class FCNet(nn.Module):\n",
        "    def __init__(self, input_size=28*28, hidden_size1=256, hidden_size2=128, output_size=10, activation=nn.ReLU()):\n",
        "        super().__init__() #  это надо помнить!\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
        "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
        "        self.fc3 = nn.Linear(hidden_size2, output_size)\n",
        "        self.activation = activation\n",
        "        self.soft_max = nn.Softmax()\n",
        "        self.flatten = nn.Flatten()\n",
        "        \n",
        "\n",
        "    def forward(self, x):  # Forward вызывается внутри метода __call__ родительского класса\n",
        "        ## x -> тензор размерности (BATCH_SIZE, N_CHANNELS, WIDTH, HEIGHT)\n",
        "        ## надо подумать над тем, что у нас полносвязные слои принимают векторы\n",
        "        out = self.flatten(x)\n",
        "        out = self.fc1(out)\n",
        "        out = self.activation(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.activation(out)\n",
        "        out = self.fc3(out)\n",
        "        logits = self.soft_max(out)\n",
        "        \n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uI0R77EQNKef"
      },
      "source": [
        "Сколько обучаемых параметров у вашей модели (весов и смещений)?\n",
        "\n",
        "**Ответ:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "MvGiuoykrzzJ"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "235146"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "(28*28+1)*256 + (256+1)*128 + (128+1)*10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwGllji2M4lp"
      },
      "source": [
        "### Задание 3. Напишите код для обучения модели\n",
        "\n",
        "**5** баллов\n",
        "\n",
        "Можно (и нужно) подглядывать в код семинара по пайторчу. Вам нужно создать модель, определить функцию потерь и оптимизатор (начнем с `SGD`). Дальше нужно обучать модель, при помощи тренировочного `Dataloader'a` и считать лосс на тренировочном и тестовом `Dataloader'ах`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Grv9xcybRfCX"
      },
      "source": [
        "Напишем функцию для рассчета `accuracy`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "9D2QPFe5JdVc"
      },
      "outputs": [],
      "source": [
        "def get_accuracy(model, dataloader):\n",
        "    \"\"\"\n",
        "    model - обученная нейронная сеть\n",
        "    dataloader - даталоадер, на котором вы хотите посчитать accuracy\n",
        "    \"\"\"\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad(): # Тензоры внутри этого блока будут иметь requires_grad=False\n",
        "        for images, labels in dataloader:\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    accuracy = correct / total\n",
        "    \n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3EmoWJyTBkE"
      },
      "source": [
        "#### Основной цикл обучения\n",
        "\n",
        "Этот код можно (и зачастую нужно) выносить в отдельную функцию, но пока что можете это не делать, все по желанию)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "uIZKSOdgUi3e"
      },
      "outputs": [],
      "source": [
        "# Создадим объект модели\n",
        "fc_net = FCNet()\n",
        "# Определим функцию потерь\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "# Создадим оптимизатор для нашей сети\n",
        "lr = 0.001 # скорость обучения\n",
        "optimizer = torch.optim.Adam(fc_net.parameters(), lr=3e-4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKYzXFqoX_fd"
      },
      "source": [
        "Напишите цикл обучения. Для начала хватит 10 эпох. Какое значение `accuracy` на тестовой выборке удалось получить?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "Ma2bshC6MxI6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/nq/kpwgbkgn0bn5tf4j40dkbk7m0000gn/T/ipykernel_2240/1517210737.py:23: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  logits = self.soft_max(out)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training completed\n"
          ]
        }
      ],
      "source": [
        "n_epochs = 10\n",
        "loss_history = []\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    epoch_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad()\n",
        "        outputs = fc_net(inputs)\n",
        "        loss = loss_function(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "    loss_history.append(epoch_loss)\n",
        "\n",
        "print('Training completed')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "2cB5LRbrS3BN"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/nq/kpwgbkgn0bn5tf4j40dkbk7m0000gn/T/ipykernel_2240/1517210737.py:23: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  logits = self.soft_max(out)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.7859"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_accuracy(fc_net, testloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HTJzBM8Yk1R"
      },
      "source": [
        "### Задание 4. Изучение влияния нормализации\n",
        "\n",
        "**3** балла\n",
        "\n",
        "Вы могли заметить, что мы забыли провести нормализацию наших данных, а для нейронных сетей это может быть очень критично.\n",
        "\n",
        "Нормализуйте данные.\n",
        "\n",
        "* Подсчитайте среднее значение и стандартное отклонение интенсивности пикселей для всех тренировочных данных\n",
        "* Нормализуйте данные с использованием этих параметров (используйте трансформацию `Normalize`)\n",
        "\n",
        "\n",
        "Оцените влияние нормировки данных."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "FHlDaYWGR6YA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[[0.00123804 0.00295836 0.00485072 0.00848344 0.01427592 0.02281055\n",
            "   0.03407695 0.04893041 0.06601468 0.08560808 0.10660776 0.12756737\n",
            "   0.14654882 0.1554954  0.14657716 0.12159222 0.09049458 0.0647362\n",
            "   0.0478952  0.03868981 0.031446   0.02399118 0.01742996 0.01143452\n",
            "   0.00676003 0.00330385 0.00167287 0.0006817 ]\n",
            "  [0.00279523 0.0059223  0.00962841 0.01574373 0.02625048 0.04317343\n",
            "   0.06538769 0.09011611 0.1159586  0.143638   0.17254597 0.19899692\n",
            "   0.21986046 0.23049453 0.22241329 0.19144584 0.14796749 0.10812113\n",
            "   0.08092457 0.066844   0.05894361 0.05068322 0.04006935 0.02815248\n",
            "   0.01731529 0.00891194 0.00393803 0.00152497]\n",
            "  [0.00471222 0.0103837  0.01648995 0.02637877 0.0427126  0.06814047\n",
            "   0.10029951 0.13356492 0.16313592 0.19168086 0.21859822 0.23913376\n",
            "   0.2540626  0.26366836 0.26208153 0.2392719  0.19518949 0.14845128\n",
            "   0.11282109 0.09105979 0.07987719 0.07131872 0.0615881  0.04916829\n",
            "   0.03411132 0.01998514 0.00977037 0.00374306]\n",
            "  [0.00717736 0.01638876 0.02548489 0.0401471  0.06347968 0.09799627\n",
            "   0.13960081 0.17863396 0.20946412 0.23455058 0.25397307 0.26856467\n",
            "   0.27982047 0.28904033 0.29043955 0.27469954 0.23490007 0.18486056\n",
            "   0.14460135 0.11787526 0.10252172 0.09240873 0.08284483 0.07112636\n",
            "   0.05469514 0.03557291 0.01871143 0.00768455]\n",
            "  [0.01047963 0.02487144 0.03727747 0.0566401  0.0867502  0.1296797\n",
            "   0.1802702  0.22343951 0.252216   0.27053815 0.28391898 0.2943092\n",
            "   0.30362907 0.31512323 0.31771627 0.30575803 0.2697977  0.2212944\n",
            "   0.18063554 0.15111668 0.13122274 0.11831953 0.10723086 0.09464156\n",
            "   0.07754792 0.05514186 0.03158213 0.01317097]\n",
            "  [0.01397052 0.03473199 0.05090896 0.07438993 0.10918447 0.15868208\n",
            "   0.21591489 0.26208454 0.2893326  0.30237713 0.31189203 0.3233908\n",
            "   0.33302766 0.34142432 0.34304336 0.33274683 0.30055362 0.25680903\n",
            "   0.21823299 0.18876728 0.16599303 0.14898288 0.13570961 0.12197214\n",
            "   0.10314061 0.07742092 0.04791723 0.02032512]\n",
            "  [0.01834301 0.04602807 0.06584986 0.09303579 0.13228002 0.18591815\n",
            "   0.2462621  0.29423097 0.3186642  0.32941586 0.33838946 0.34963343\n",
            "   0.3589425  0.3640294  0.36295855 0.35353082 0.32504502 0.28597218\n",
            "   0.2508765  0.22290593 0.19986838 0.18239576 0.16846387 0.15305161\n",
            "   0.13248128 0.10420877 0.06722061 0.0289463 ]\n",
            "  [0.02223929 0.05823139 0.0839825  0.11636218 0.15948504 0.21589036\n",
            "   0.2760384  0.32049692 0.34369335 0.35281458 0.35894862 0.3678986\n",
            "   0.37417325 0.37395018 0.36955997 0.36072063 0.33716455 0.30275652\n",
            "   0.2717431  0.24596709 0.22495933 0.20955524 0.19734937 0.18341209\n",
            "   0.16384654 0.1345286  0.09074469 0.03965671]\n",
            "  [0.0247674  0.06931816 0.10305553 0.14155799 0.19027679 0.2501279\n",
            "   0.30576766 0.34401643 0.3627017  0.3698736  0.37585768 0.37944523\n",
            "   0.37714264 0.3670761  0.35938305 0.3567116  0.33854222 0.30789155\n",
            "   0.27934125 0.2551783  0.23896335 0.22871007 0.22103454 0.21055539\n",
            "   0.19477747 0.16603355 0.11862254 0.05396997]\n",
            "  [0.02657188 0.07723875 0.11912403 0.16634908 0.2234201  0.28424835\n",
            "   0.33397496 0.36384666 0.3754389  0.3803669  0.3861254  0.38433006\n",
            "   0.37124044 0.35170263 0.3429765  0.34608006 0.33599284 0.30858272\n",
            "   0.27895498 0.25473365 0.23964456 0.23324063 0.23115595 0.22931497\n",
            "   0.22292817 0.20041789 0.14916779 0.0716386 ]\n",
            "  [0.02775488 0.08166125 0.13013491 0.18389553 0.24556582 0.306847\n",
            "   0.35203928 0.3759067  0.38239035 0.3831439  0.3855555  0.37972152\n",
            "   0.36056113 0.33761403 0.33031762 0.33645573 0.33172458 0.30698165\n",
            "   0.2786234  0.25264233 0.23300122 0.22430171 0.22585747 0.23359191\n",
            "   0.2425884  0.23208912 0.18070166 0.09100599]\n",
            "  [0.02796882 0.08253768 0.13382056 0.18968701 0.2514205  0.31202078\n",
            "   0.354154   0.37412223 0.37586963 0.37177145 0.37009874 0.36379066\n",
            "   0.3491187  0.3302324  0.32369903 0.33317038 0.3321071  0.31033313\n",
            "   0.2811541  0.2517724  0.22499368 0.20894322 0.21116015 0.22815937\n",
            "   0.25124398 0.25440925 0.20777293 0.10900978]\n",
            "  [0.02750838 0.08016913 0.12972268 0.18390806 0.24530219 0.30402473\n",
            "   0.3435677  0.36139354 0.35949636 0.3529299  0.35031402 0.34880546\n",
            "   0.33998802 0.32588798 0.32340267 0.33541295 0.336045   0.3163114\n",
            "   0.28539908 0.25032893 0.21595524 0.1941781  0.19631    0.21913338\n",
            "   0.25279617 0.26615906 0.22512352 0.12202159]\n",
            "  [0.02693863 0.07770921 0.12395393 0.17553626 0.23477735 0.29183757\n",
            "   0.32895106 0.34430432 0.340827   0.334503   0.33529934 0.33683178\n",
            "   0.3311631  0.3223423  0.32507443 0.33942595 0.3412334  0.32190958\n",
            "   0.28833076 0.24716578 0.20879287 0.18444023 0.18502282 0.20991036\n",
            "   0.24900809 0.26615152 0.22933944 0.12627277]\n",
            "  [0.02732193 0.07734521 0.12194515 0.17178148 0.22941323 0.28045082\n",
            "   0.31095502 0.32417828 0.32386976 0.3237358  0.32656142 0.32795674\n",
            "   0.3248991  0.32203904 0.33249903 0.34696695 0.34430346 0.3232185\n",
            "   0.2881354  0.24516594 0.20496026 0.17864855 0.17869583 0.20269747\n",
            "   0.23921144 0.25445607 0.21863197 0.12059276]\n",
            "  [0.02757757 0.07820904 0.12410776 0.17611924 0.23029254 0.27052566\n",
            "   0.2932229  0.30461186 0.30913985 0.31413454 0.3218056  0.32601026\n",
            "   0.32544306 0.33052087 0.34465885 0.3540672  0.34339166 0.31698442\n",
            "   0.28291166 0.24217273 0.20253533 0.17622991 0.17487523 0.19481274\n",
            "   0.22283356 0.23078957 0.19425707 0.10616411]\n",
            "  [0.02837466 0.08257019 0.1340679  0.18735284 0.23367931 0.262804\n",
            "   0.2779358  0.28653145 0.2932203  0.30160427 0.31329942 0.32318217\n",
            "   0.33274132 0.34349748 0.3570565  0.35739458 0.33509886 0.3023557\n",
            "   0.26968482 0.23456573 0.20054612 0.17778105 0.17483054 0.18704657\n",
            "   0.20231898 0.19927563 0.16136849 0.08489228]\n",
            "  [0.03054999 0.09234251 0.14999576 0.20134619 0.23737213 0.25696567\n",
            "   0.26639986 0.2704743  0.27538416 0.28527844 0.29976127 0.31785277\n",
            "   0.340375   0.3572651  0.36577165 0.35525677 0.32298264 0.28634104\n",
            "   0.25459662 0.2271834  0.20251377 0.1844894  0.17829657 0.18071517\n",
            "   0.18066823 0.16403775 0.12497497 0.06203522]\n",
            "  [0.03544923 0.10723101 0.16605045 0.2097866  0.23682219 0.25003722\n",
            "   0.25529286 0.25824368 0.26200432 0.27015436 0.2874586  0.3146051\n",
            "   0.34407485 0.36167926 0.36537173 0.3471897  0.31094983 0.2716498\n",
            "   0.24195434 0.22116426 0.20521675 0.19258705 0.18368484 0.17431574\n",
            "   0.15768968 0.12964797 0.09083248 0.042755  ]\n",
            "  [0.04222219 0.11951757 0.17241156 0.20667477 0.22801428 0.23950115\n",
            "   0.24642596 0.2525899  0.25731567 0.26278254 0.27978185 0.30870378\n",
            "   0.3384816  0.35475177 0.35391918 0.3344717  0.2971647  0.25951326\n",
            "   0.23327251 0.21635148 0.20536011 0.19686197 0.18518315 0.16444625\n",
            "   0.13514115 0.10128957 0.0664714  0.03035446]\n",
            "  [0.04543379 0.11982454 0.1634684  0.19095118 0.21005507 0.22568513\n",
            "   0.23928873 0.25122514 0.25656614 0.25794128 0.27087715 0.29503053\n",
            "   0.3210095  0.33626077 0.33502242 0.31765166 0.28513613 0.25308162\n",
            "   0.22853944 0.21217385 0.20084238 0.1911813  0.17513049 0.14808851\n",
            "   0.11408133 0.07967643 0.04929163 0.02235392]\n",
            "  [0.0403636  0.10459171 0.14048499 0.16581036 0.18763046 0.20963244\n",
            "   0.22968778 0.24457552 0.2482804  0.24745646 0.25565264 0.27705032\n",
            "   0.30076644 0.31702185 0.3179586  0.30376825 0.27738142 0.25104114\n",
            "   0.22766566 0.20708553 0.18976857 0.17343724 0.15492721 0.1272447\n",
            "   0.09521306 0.0639606  0.03808919 0.01764651]\n",
            "  [0.03061319 0.08224149 0.11447653 0.13840134 0.16295636 0.18772969\n",
            "   0.20949496 0.22340637 0.2287894  0.23189715 0.23982136 0.26012453\n",
            "   0.2853673  0.30383685 0.30526578 0.29344356 0.27313352 0.25018877\n",
            "   0.22621524 0.20007217 0.1742281  0.1520121  0.1316162  0.10640886\n",
            "   0.07981583 0.05322557 0.03104542 0.01466989]\n",
            "  [0.02179489 0.06088216 0.08932307 0.11246797 0.13520816 0.15911919\n",
            "   0.17840828 0.1908223  0.20233043 0.21464863 0.22767587 0.25055987\n",
            "   0.27741632 0.2949915  0.29659817 0.2875573  0.27106535 0.24936241\n",
            "   0.2221119  0.19055349 0.15985835 0.13275939 0.10954978 0.08812547\n",
            "   0.06610361 0.04461998 0.02597387 0.01178594]\n",
            "  [0.01528853 0.0438123  0.06779574 0.08768285 0.10616552 0.12626475\n",
            "   0.142975   0.1559289  0.17371581 0.19653612 0.22158107 0.25024015\n",
            "   0.27718368 0.29531687 0.29982105 0.2941836  0.27866685 0.25336596\n",
            "   0.22088136 0.18338142 0.14763775 0.11639909 0.09242171 0.07244892\n",
            "   0.0542926  0.03780368 0.02202108 0.00971764]\n",
            "  [0.01016712 0.03041492 0.04870355 0.06451998 0.07814047 0.09265883\n",
            "   0.107606   0.12348728 0.14367676 0.17277263 0.20726645 0.24264665\n",
            "   0.27567273 0.3008078  0.3151757  0.31245276 0.29349464 0.26209486\n",
            "   0.22068734 0.17689112 0.13524513 0.10218272 0.07767156 0.05862333\n",
            "   0.04330605 0.03044513 0.01799175 0.00792846]\n",
            "  [0.00566249 0.01846999 0.03166074 0.04277959 0.05199586 0.06061867\n",
            "   0.07219724 0.08706579 0.10643392 0.13123402 0.16501364 0.20293836\n",
            "   0.24045807 0.27388203 0.29563123 0.29455173 0.2739387  0.24002104\n",
            "   0.19784215 0.15270367 0.11172327 0.08024113 0.05816849 0.04276441\n",
            "   0.03124855 0.02160166 0.01230571 0.00489562]\n",
            "  [0.00195967 0.00806141 0.01515633 0.0214264  0.02533817 0.03001908\n",
            "   0.03659208 0.04580549 0.0579119  0.07425828 0.09641451 0.1256175\n",
            "   0.16010709 0.1944724  0.21571743 0.21638684 0.19928715 0.1700117\n",
            "   0.13534662 0.09998386 0.06969573 0.04731392 0.03297614 0.02360699\n",
            "   0.01717582 0.01165991 0.00635178 0.00198124]]] [[[0.02492679 0.04134501 0.0535747  0.072441   0.09452478 0.11872246\n",
            "   0.14448628 0.17272173 0.19955702 0.22576481 0.24972959 0.268838\n",
            "   0.28619358 0.29387563 0.28675497 0.26431382 0.23077671 0.19804502\n",
            "   0.17092796 0.15428127 0.13885392 0.12068716 0.10288195 0.08308209\n",
            "   0.06410759 0.04414    0.03108015 0.01881723]\n",
            "  [0.03950964 0.06081619 0.07913615 0.10252605 0.13230997 0.17001793\n",
            "   0.20852369 0.2420943  0.27008766 0.29671282 0.32046318 0.34035772\n",
            "   0.35589862 0.364805   0.361608   0.34138885 0.3061572  0.26649332\n",
            "   0.23221499 0.21457897 0.20342657 0.19034782 0.16936809 0.14170294\n",
            "   0.11023571 0.07640207 0.0487352  0.0301194 ]\n",
            "  [0.05113984 0.08306398 0.1055769  0.13492699 0.17133541 0.21468486\n",
            "   0.25840053 0.29366904 0.31785914 0.3383974  0.35705858 0.36884448\n",
            "   0.3785895  0.38593277 0.38665783 0.37469766 0.34627357 0.3088992\n",
            "   0.27259335 0.24814202 0.23580472 0.22624825 0.21298438 0.1919588\n",
            "   0.15926364 0.12071439 0.08187936 0.04684794]\n",
            "  [0.06114937 0.10427875 0.13217302 0.16681528 0.20864603 0.25592032\n",
            "   0.3010045  0.33244973 0.3519058  0.36688596 0.37781382 0.3872586\n",
            "   0.39309257 0.39790803 0.39757946 0.39023077 0.36918974 0.33576235\n",
            "   0.30319342 0.27858746 0.26452965 0.25385877 0.24429606 0.22989792\n",
            "   0.20254402 0.16245623 0.11573625 0.06958847]\n",
            "  [0.0729352  0.12928186 0.16094714 0.1981706  0.24367693 0.2919928\n",
            "   0.33608815 0.3637413  0.37618348 0.38336775 0.39140594 0.39757773\n",
            "   0.40148342 0.4060123  0.4054689  0.40073016 0.38364276 0.35829574\n",
            "   0.33265826 0.31009352 0.29424113 0.28366733 0.27439454 0.2607977\n",
            "   0.23899141 0.20264725 0.15091173 0.09175453]\n",
            "  [0.08309668 0.1540963  0.18878926 0.22765882 0.27179867 0.31912962\n",
            "   0.36042887 0.38481987 0.39349115 0.39614913 0.40141037 0.40675133\n",
            "   0.40919125 0.4118549  0.41133082 0.4072622  0.39537242 0.37579903\n",
            "   0.35654616 0.3388258  0.32435128 0.31155503 0.30205974 0.290224\n",
            "   0.27164984 0.2389005  0.187925   0.11426324]\n",
            "  [0.0945076  0.17814311 0.21461216 0.2533899  0.29698414 0.34175745\n",
            "   0.3786572  0.3995411  0.40407872 0.40505594 0.4081678  0.4127177\n",
            "   0.4145336  0.41540346 0.41433653 0.41143396 0.40275046 0.38759884\n",
            "   0.3727574  0.3603955  0.34876004 0.3387256  0.33015686 0.3192117\n",
            "   0.30323485 0.27453598 0.22230797 0.13529691]\n",
            "  [0.1028411  0.20105436 0.24215262 0.28193375 0.3222546  0.36163273\n",
            "   0.3932126  0.409038   0.4127393  0.41255668 0.41263047 0.4141456\n",
            "   0.41522238 0.4148646  0.41407123 0.4126515  0.40696812 0.3935814\n",
            "   0.38215688 0.37255168 0.3641285  0.35770458 0.3515913  0.34381035\n",
            "   0.33187696 0.30801415 0.25628603 0.15867652]\n",
            "  [0.10722963 0.2178893  0.26659957 0.30734795 0.34558123 0.3812808\n",
            "   0.40498552 0.41481084 0.41779703 0.4162774  0.4142331  0.41384172\n",
            "   0.4119232  0.41020283 0.41011783 0.4121967  0.40827513 0.3967708\n",
            "   0.38594484 0.37676767 0.3712284  0.3688067  0.36758202 0.36246654\n",
            "   0.35507092 0.3370953  0.2913404  0.18479566]\n",
            "  [0.11065052 0.22807643 0.2840843  0.32896563 0.36786708 0.39865214\n",
            "   0.41443983 0.42058402 0.41967258 0.41698074 0.4140179  0.4110778\n",
            "   0.40870506 0.40489873 0.40463585 0.41066027 0.41015762 0.39981398\n",
            "   0.38696724 0.37620226 0.37115517 0.372179   0.37433946 0.37521955\n",
            "   0.37325564 0.36337313 0.32284692 0.21283352]\n",
            "  [0.11344216 0.23489651 0.29599518 0.3433873  0.3818318  0.40796214\n",
            "   0.42072657 0.4236657  0.42030114 0.41628966 0.41317266 0.40994763\n",
            "   0.40632054 0.40234518 0.4026931  0.41010278 0.4106981  0.40085423\n",
            "   0.38777897 0.37635997 0.3674889  0.366751   0.3709521  0.37878713\n",
            "   0.38658082 0.38584185 0.35096237 0.23828584]\n",
            "  [0.1156101  0.23757744 0.30135018 0.34841046 0.3854002  0.4108358\n",
            "   0.4226161  0.42401952 0.41937998 0.41310224 0.4097866  0.40708783\n",
            "   0.4052045  0.40239814 0.40271607 0.40996113 0.41119793 0.40223533\n",
            "   0.39082834 0.37712112 0.36293226 0.35580447 0.3613139  0.37541705\n",
            "   0.39279747 0.40117666 0.37237388 0.25917244]\n",
            "  [0.1160063  0.2355566  0.29719454 0.34439382 0.381563   0.40763813\n",
            "   0.42045498 0.42225078 0.4166135  0.40981567 0.40682533 0.40628645\n",
            "   0.40559056 0.40361807 0.40375477 0.41067448 0.4114709  0.40421945\n",
            "   0.3931436  0.37680796 0.35774428 0.34583834 0.35210404 0.36999655\n",
            "   0.39439768 0.40942484 0.38638386 0.27287328]\n",
            "  [0.1153084  0.23384312 0.29286197 0.33823368 0.37576866 0.40485993\n",
            "   0.4188429  0.42017594 0.41357043 0.4067898  0.40448996 0.40558612\n",
            "   0.40584654 0.40415934 0.40624443 0.41173637 0.4118     0.40579924\n",
            "   0.39404523 0.37618542 0.3548329  0.33892736 0.34279168 0.36303598\n",
            "   0.39282405 0.4114122  0.3908735  0.27650717]\n",
            "  [0.11670331 0.23493384 0.29218838 0.33554608 0.37416288 0.4021484\n",
            "   0.41429093 0.415346   0.4101999  0.4051969  0.40465036 0.40585297\n",
            "   0.40594512 0.40634936 0.40930155 0.4138746  0.41234004 0.40558887\n",
            "   0.394311   0.3758077  0.35347787 0.33553845 0.3374678  0.35855007\n",
            "   0.38806024 0.40601578 0.38406324 0.27155286]\n",
            "  [0.11849327 0.23583528 0.29439035 0.33950388 0.37706086 0.39995965\n",
            "   0.4086771  0.40914565 0.4064028  0.40241352 0.40457246 0.40663913\n",
            "   0.4069531  0.41029787 0.41419575 0.4163148  0.41321436 0.40419796\n",
            "   0.3926321  0.3753423  0.35225096 0.333049   0.33440283 0.35325047\n",
            "   0.37921196 0.3919275  0.36586723 0.25647938]\n",
            "  [0.11909822 0.24115977 0.30392966 0.34981018 0.38216746 0.39808598\n",
            "   0.4022131  0.40139487 0.39958876 0.39876613 0.40234503 0.4078241\n",
            "   0.41113216 0.4143193  0.4171742  0.41790232 0.41130245 0.39926895\n",
            "   0.38813624 0.37220344 0.35193563 0.335473   0.33494613 0.3484656\n",
            "   0.3655545  0.3690897  0.33740097 0.23038636]\n",
            "  [0.12344028 0.25367686 0.31939304 0.36312294 0.3871572  0.39582115\n",
            "   0.39649618 0.39352503 0.39058676 0.39062053 0.39727992 0.405649\n",
            "   0.4143067  0.41760176 0.42012256 0.41701216 0.40773013 0.39439434\n",
            "   0.38234362 0.37006176 0.35562438 0.34173527 0.33882508 0.3445516\n",
            "   0.34950972 0.3388701  0.29949942 0.1978265 ]\n",
            "  [0.1315171  0.27196386 0.3364936  0.3716865  0.38756573 0.39161116\n",
            "   0.38936454 0.38575694 0.38257164 0.3832458  0.39190245 0.40407073\n",
            "   0.4149256  0.4187091  0.41970924 0.414782   0.40456674 0.3896547\n",
            "   0.37665573 0.36695045 0.35810313 0.3499689  0.34484    0.33989632\n",
            "   0.3287014  0.30350956 0.25578368 0.16427325]\n",
            "  [0.14407092 0.2890009  0.34494162 0.3699228  0.38096282 0.38347855\n",
            "   0.38333663 0.3811554  0.3792744  0.3799443  0.3882013  0.40062004\n",
            "   0.41214135 0.41655833 0.41585925 0.41133076 0.3999889  0.38352838\n",
            "   0.37202486 0.36318213 0.3584998  0.35455552 0.34751916 0.3317982\n",
            "   0.30609912 0.26902908 0.21849824 0.13711864]\n",
            "  [0.15073977 0.29175088 0.33717567 0.35678974 0.36716586 0.37463677\n",
            "   0.37798655 0.38004443 0.3796345  0.37883067 0.38447657 0.39509708\n",
            "   0.4059657  0.41151968 0.4117502  0.4068399  0.39543855 0.38091803\n",
            "   0.36855054 0.35960847 0.35455257 0.3501787  0.34020528 0.31765026\n",
            "   0.28283846 0.23909615 0.1877865  0.11826983]\n",
            "  [0.14245653 0.2732708  0.31447873 0.3355255  0.351168   0.36361894\n",
            "   0.37253448 0.37779337 0.37631285 0.37510493 0.37822962 0.3883038\n",
            "   0.3984182  0.4062924  0.4084425  0.40352798 0.39286268 0.37997517\n",
            "   0.36698923 0.3563452  0.34582713 0.3356334  0.32275513 0.2976314\n",
            "   0.26049742 0.21511535 0.1648815  0.10507952]\n",
            "  [0.12425804 0.2431274  0.28762725 0.31056625 0.33177233 0.3481235\n",
            "   0.3605603  0.3671518  0.3677651  0.36717948 0.37116233 0.38199237\n",
            "   0.394225   0.40314022 0.404266   0.39992404 0.39110196 0.38058943\n",
            "   0.36788955 0.3506758  0.3337696  0.31717512 0.30027288 0.27422363\n",
            "   0.24063747 0.19700608 0.1486755  0.09611841]\n",
            "  [0.10527048 0.21064702 0.25691077 0.28552192 0.30713972 0.32687458\n",
            "   0.33954158 0.34721354 0.3533515  0.3598624  0.36630958 0.3784465\n",
            "   0.39049828 0.3977779  0.39881074 0.39607668 0.38899508 0.37921277\n",
            "   0.36441195 0.34426746 0.32232115 0.29963353 0.27666634 0.2516786\n",
            "   0.21996477 0.18105896 0.13617434 0.08506018]\n",
            "  [0.08911838 0.18036291 0.22589028 0.25574946 0.27698085 0.29651284\n",
            "   0.31115407 0.32185453 0.3345607  0.3497809  0.36473924 0.3796522\n",
            "   0.391832   0.39715186 0.3971956  0.39469394 0.38919118 0.37942213\n",
            "   0.3635579  0.3395749  0.31214604 0.28327274 0.25673825 0.22996464\n",
            "   0.20110294 0.16898918 0.12673803 0.07737876]\n",
            "  [0.07278851 0.1502538  0.19265567 0.22100466 0.24128251 0.25813723\n",
            "   0.2757785  0.2925841  0.31047332 0.33451378 0.3585417  0.37904313\n",
            "   0.39298397 0.40165165 0.4069656  0.40596637 0.40015644 0.3880648\n",
            "   0.3668618  0.33856097 0.3031212  0.26857162 0.2375341  0.20822309\n",
            "   0.18064608 0.15165405 0.11451295 0.0707045 ]\n",
            "  [0.05322536 0.11521839 0.15394041 0.17901675 0.19564588 0.2086819\n",
            "   0.2278552  0.24921352 0.27299485 0.29805923 0.32761893 0.35539263\n",
            "   0.37751824 0.3950252  0.40534756 0.4066048  0.39899004 0.38303757\n",
            "   0.3571433  0.32124814 0.28016952 0.23967905 0.20567602 0.17752512\n",
            "   0.15216465 0.1270971  0.09331629 0.05375885]\n",
            "  [0.02832272 0.06978326 0.09828649 0.11820079 0.12659602 0.13864706\n",
            "   0.15364997 0.171316   0.19250616 0.216426   0.24377598 0.27499217\n",
            "   0.30587193 0.33299547 0.34935424 0.35104752 0.34225956 0.32253182\n",
            "   0.29314822 0.25600794 0.21579324 0.17782575 0.1487916  0.12540092\n",
            "   0.10666332 0.08740328 0.06255381 0.03010883]]]\n"
          ]
        }
      ],
      "source": [
        "mean = np.mean([sample[0].numpy() for sample in trainset], axis=0)\n",
        "std = np.std([sample[0].numpy() for sample in trainset], axis=0)\n",
        "print(mean, std)\n",
        "\n",
        "transform_with_norm = torchvision.transforms.Compose([\n",
        "                torchvision.transforms.ToTensor(),\n",
        "                torchvision.transforms.Normalize(mean, std + 1e-6)\n",
        "            ])\n",
        "\n",
        "trainset.transform = transform_with_norm \n",
        "testset.transform = transform_with_norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "Qj93J3X_R6aa"
      },
      "outputs": [],
      "source": [
        "fc_net = FCNet()\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "lr = 0.001\n",
        "optimizer = torch.optim.Adam(fc_net.parameters(), lr=3e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "VWZtYBCvAoWQ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/nq/kpwgbkgn0bn5tf4j40dkbk7m0000gn/T/ipykernel_2240/1517210737.py:23: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  logits = self.soft_max(out)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training completed\n"
          ]
        }
      ],
      "source": [
        "n_epochs = 10\n",
        "loss_history = []\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    epoch_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad()\n",
        "        outputs = fc_net(inputs)\n",
        "        loss = loss_function(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "    loss_history.append(epoch_loss)\n",
        "\n",
        "print('Training completed')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "Gfbv9OIAAoYT"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/nq/kpwgbkgn0bn5tf4j40dkbk7m0000gn/T/ipykernel_2240/1517210737.py:23: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  logits = self.soft_max(out)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.8175"
            ]
          },
          "execution_count": 91,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_accuracy(fc_net, testloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcIJvhWkcjlh"
      },
      "source": [
        "Как изменилась `accuracy` после нормализации?"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Стала чуть побольше"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atcfzu4acxP2"
      },
      "source": [
        "### Задание 5. Изучение влияния функции активации\n",
        "\n",
        "**3** балла\n",
        "\n",
        "Исследуйте влияние функций активации на скорость обучения и точность предсказаний модели.\n",
        "\n",
        "Используйте три функции:\n",
        "\n",
        "* [Sigmoid](https://pytorch.org/docs/stable/nn.functional.html#sigmoid)\n",
        "* [GELU](https://pytorch.org/docs/stable/nn.functional.html#gelu)\n",
        "* [Tanh](https://pytorch.org/docs/stable/generated/torch.nn.Tanh.html#torch.nn.Tanh)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "bAESPpjGa3M1"
      },
      "outputs": [],
      "source": [
        "fc_net = FCNet(activation=nn.Sigmoid())\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "lr = 0.001\n",
        "optimizer = torch.optim.Adam(fc_net.parameters(), lr=3e-4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "GdvHSFeKa2sW"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/nq/kpwgbkgn0bn5tf4j40dkbk7m0000gn/T/ipykernel_2240/3483279793.py:23: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  logits = self.soft_max(out)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training completed\n"
          ]
        }
      ],
      "source": [
        "n_epochs = 10\n",
        "loss_history = []\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    epoch_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad()\n",
        "        outputs = fc_net(inputs)\n",
        "        loss = loss_function(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "    loss_history.append(epoch_loss)\n",
        "\n",
        "print('Training completed')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "zBok8pXNa2vA"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/nq/kpwgbkgn0bn5tf4j40dkbk7m0000gn/T/ipykernel_2240/3483279793.py:23: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  logits = self.soft_max(out)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.7473"
            ]
          },
          "execution_count": 98,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_accuracy(fc_net, testloader)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sigmoid accuracy = 0.7473, training time = 1m15s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [],
      "source": [
        "fc_net = FCNet(activation=nn.GELU())\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "lr = 0.001\n",
        "optimizer = torch.optim.Adam(fc_net.parameters(), lr=3e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/nq/kpwgbkgn0bn5tf4j40dkbk7m0000gn/T/ipykernel_2240/3483279793.py:23: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  logits = self.soft_max(out)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training completed\n"
          ]
        }
      ],
      "source": [
        "n_epochs = 10\n",
        "loss_history = []\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    epoch_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad()\n",
        "        outputs = fc_net(inputs)\n",
        "        loss = loss_function(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "    loss_history.append(epoch_loss)\n",
        "\n",
        "print('Training completed')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/nq/kpwgbkgn0bn5tf4j40dkbk7m0000gn/T/ipykernel_2240/3483279793.py:23: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  logits = self.soft_max(out)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.8705"
            ]
          },
          "execution_count": 101,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_accuracy(fc_net, testloader)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "GELU accuracy = 0.8705, training time = 1m21s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {},
      "outputs": [],
      "source": [
        "fc_net = FCNet(activation=nn.Tanh())\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "lr = 0.001\n",
        "optimizer = torch.optim.Adam(fc_net.parameters(), lr=3e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/nq/kpwgbkgn0bn5tf4j40dkbk7m0000gn/T/ipykernel_2240/3483279793.py:23: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  logits = self.soft_max(out)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training completed\n"
          ]
        }
      ],
      "source": [
        "n_epochs = 10\n",
        "loss_history = []\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    epoch_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad()\n",
        "        outputs = fc_net(inputs)\n",
        "        loss = loss_function(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "    loss_history.append(epoch_loss)\n",
        "\n",
        "print('Training completed')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/nq/kpwgbkgn0bn5tf4j40dkbk7m0000gn/T/ipykernel_2240/3483279793.py:23: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  logits = self.soft_max(out)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.8557"
            ]
          },
          "execution_count": 107,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_accuracy(fc_net, testloader)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tanh accuracy = 0.8557, training time = 1m11s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aG2Oyxy2egVV"
      },
      "source": [
        "С использованием какой функции активации удалось досчить наибольшей `accuracy`?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SaqWhlkjuO3"
      },
      "source": [
        "GELU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20Ls3Bfsifqd"
      },
      "source": [
        "### Задание 6. Другие оптимизаторы\n",
        "\n",
        "**4** балла\n",
        "\n",
        "Исследуйте влияние оптимизаторов на скорость обучения и точность предсказаний модели.\n",
        "\n",
        "Попробуйте следующие:\n",
        "\n",
        "* [Adam](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam)\n",
        "* [RMSprop](https://pytorch.org/docs/stable/generated/torch.optim.RMSprop.html#torch.optim.RMSprop)\n",
        "* [Adagrad](https://pytorch.org/docs/stable/generated/torch.optim.Adagrad.html#torch.optim.Adagrad)\n",
        "\n",
        "Вам нужно снова обучить 3 модели и сравнить их перформанс (функцию активации используйте ту, которая показала себя лучше всего)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "rzL2LdA-ifJh"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/nq/kpwgbkgn0bn5tf4j40dkbk7m0000gn/T/ipykernel_2240/3483279793.py:23: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  logits = self.soft_max(out)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training completed\n",
            "RMSprop accuracy: 0.8779\n"
          ]
        }
      ],
      "source": [
        "fc_net = FCNet(activation=nn.GELU())\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "lr = 0.001\n",
        "optimizer = torch.optim.RMSprop(fc_net.parameters(), lr=3e-4)\n",
        "\n",
        "n_epochs = 10\n",
        "loss_history = []\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    epoch_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad()\n",
        "        outputs = fc_net(inputs)\n",
        "        loss = loss_function(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "    loss_history.append(epoch_loss)\n",
        "\n",
        "print('Training completed')\n",
        "print(\"RMSprop accuracy:\", get_accuracy(fc_net, testloader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/nq/kpwgbkgn0bn5tf4j40dkbk7m0000gn/T/ipykernel_2240/3483279793.py:23: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  logits = self.soft_max(out)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training completed\n",
            "Adagrad accuracy: 0.6387\n"
          ]
        }
      ],
      "source": [
        "fc_net = FCNet(activation=nn.GELU())\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "lr = 0.001\n",
        "optimizer = torch.optim.Adagrad(fc_net.parameters(), lr=3e-4)\n",
        "\n",
        "n_epochs = 10\n",
        "loss_history = []\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    epoch_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad()\n",
        "        outputs = fc_net(inputs)\n",
        "        loss = loss_function(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "    loss_history.append(epoch_loss)\n",
        "\n",
        "print('Training completed')\n",
        "print(\"Adagrad accuracy:\", get_accuracy(fc_net, testloader))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Лучший accuracy у RMSprop (0.8779), чучуть хуже у Adam (0.8705), у Adagrad совсем не оч (0.6387). По времени все примерно равны"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHA48PsperxS"
      },
      "source": [
        "### Задание 7. Реализация ReLU\n",
        "\n",
        "**4** балла\n",
        "\n",
        "Самостоятельно реализуйте функцию активации ReLU.\n",
        "Замените в уже обученной модели функцию активации на вашу. Убедитесь что ничего не изменилась."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "63uTkUp-a2xr"
      },
      "outputs": [],
      "source": [
        "class CustomReLU(nn.Module):     \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # YOUR CODE HERE\n",
        "        # если элемент x < 0, то 0, если >= 0, то x\n",
        "        x = torch.max(x, torch.tensor(0.0))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsKzxa33fhbN"
      },
      "source": [
        "Заново обучите модель и проверьте правильность реализации `CustomReLU`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "ePP55RBeecYh"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/nq/kpwgbkgn0bn5tf4j40dkbk7m0000gn/T/ipykernel_2240/3483279793.py:23: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  logits = self.soft_max(out)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training completed\n",
            "0.8146\n"
          ]
        }
      ],
      "source": [
        "fc_net = FCNet(activation=CustomReLU())\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "lr = 0.001\n",
        "optimizer = torch.optim.Adam(fc_net.parameters(), lr=3e-4)\n",
        "\n",
        "n_epochs = 10\n",
        "loss_history = []\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    epoch_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad()\n",
        "        outputs = fc_net(inputs)\n",
        "        loss = loss_function(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "    loss_history.append(epoch_loss)\n",
        "\n",
        "print('Training completed')\n",
        "print(get_accuracy(fc_net, testloader))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Получилось примерно столько же и по времени, и по accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWBG1mMwgN17"
      },
      "source": [
        "### Задание 8. Генерация картинок\n",
        "\n",
        "**3** балла\n",
        "\n",
        "Так как вы снова работаете в командах, то придумайте 3 предложения и сгенерируйте при помощи них 3 картинки, используя телеграм бота [ruDALLE](https://t.me/sber_rudalle_xl_bot). Прикрепите сюда ваши картины."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hlqgll9qecdZ"
      },
      "source": [
        "![alt text for screen readers](images/image1.jpg \"Text to show on mouseover\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\"Студент делает дз в час ночи\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![alt text for screen readers](images/image2.jpg \"Text to show on mouseover\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\"Ночь. Улица. Фонарь. Аптека.\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![alt text for screen readers](images/image3.jpg \"Text to show on mouseover\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\"Буратино утонул\""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNNR6Jf1C4Xx0J5iQF4rOen",
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "FC_NN_practice.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
